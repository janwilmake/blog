<!DOCTYPE html>
<html>
<head>
<title>llm-voice-calls.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="implementing-llm-voice-calls">Implementing LLM Voice Calls</h1>
<p>An idea that has lingered in my head ever since ChatGPT came out was that of LLM voice calls. Imagine being able to simply call your AI from your Nokia 3310 and being able to ask it to do anything for you. As an active traveler that often goes to remote places for adventuring, I'd love such an interface with the highest intelligence of currenet society. It's not always possible to have a computer with a huge battery, or even an internet connection.</p>
<p>On a sunny sunday in the Netherlands, I decided to take a closer look to the possibility of this magical idea.</p>
<p>I've recently migrated my entire codebase to Bun.sh. For WhatsApp and SMS integration I have already used Twilio. Therefore, I quickly came to these two guides:</p>
<ol>
<li><a href="https://www.twilio.com/docs/voice/twiml/stream">Voice call streams</a></li>
<li><a href="https://bun.sh/docs/api/websockets">Bun.sh WebSockets</a></li>
</ol>
<p>But this is next level. Let's take a step back first. We need to first implement voice calls in the first place. After reading the twilio docs I came to these steps that I could take:</p>
<p>Step 1 is to initiate the call:</p>
<ul>
<li><a href="https://www.twilio.com/docs/voice/tutorials/how-to-make-outbound-phone-calls/node">Make an outbound call</a></li>
<li><a href="https://www.twilio.com/docs/voice/tutorials/how-to-respond-to-incoming-phone-calls/node">Respond to incoming call</a></li>
</ul>
<p>Step 2 is to have a conversation in realtime:</p>
<ul>
<li><a href="https://www.twilio.com/docs/voice/twiml/stream">Stream the incoming audio</a></li>
<li><a href="https://www.twilio.com/docs/voice/twiml/play">Play an audio file to the call</a></li>
</ul>
<p>Optionally, we can do more things like this:</p>
<ul>
<li><a href="https://www.twilio.com/docs/voice/twiml/record">Record the call</a></li>
<li><a href="https://www.twilio.com/docs/voice/twiml/dial">Add another device into the call</a></li>
</ul>
<p>A couple days ago I implemented a regular endpoint to receive a call (<code>receiveTwilioCallWithContextRaw</code>) and to send one (<code>sendPhoneCallMessage</code>). This is basically step 1. It works sufficiently well for one-time messages. I've integrated this into KingOS to make it possible to call anyone in your network with a phone call. The person that picks up will receive a generated voice that says a message. After that, the person can reply something and hang up the phone. The response is then sent to a webhook to process with Whisper, and the message is sent as audio + text back into the chat. I'm very happy with this simple implemenatation.</p>
<h1 id="latency">Latency</h1>
<p><img src="./llm-voice-calls.drawio.png" alt=""></p>
<p>I drew this graph to make it clearer where the voice is going before another voice returns to the same user. As you can see, the voice needs to first come from the phone of the other end all the way to the USA. It then goes through 3 stages:</p>
<ol>
<li>
<p><strong>STT (Speech to Text)</strong>: We can do this with either OpenAI's Whisper API, or maybe something like <a href="https://www.rev.ai">Rev AI</a> or <a href="https://deepgram.com">DeepGram</a> which is more performant for realtime transcription. Depending on this choice the speed may vary a lot.</p>
</li>
<li>
<p><strong>LLM (Large Language Model Transformation)</strong>: We can use the OpenAI gpt-3.5-turbo streaming endpoint (chat completion). This gives us a first word within 250ms on average, and an entire sentence within 750ms most of the time.</p>
</li>
<li>
<p><strong>TTS (Text to Speech)</strong>: For this we can use <a href="https://play.ht">play.ht</a> for realtime voice generation. This is super high quality which is certainly the most fun. However, if we want to go for speed, we can also use local cli tools like <code>say</code> for macos and <code>espeak</code> for linux. This would reduce the time required by a second with ease (it's near-instant). The quality is a lot lower though.</p>
</li>
</ol>
<p>Each of them has a latency, which is important to note. If you add everything together, we can expect to have <strong>at least 1.5 seconds latency</strong> before we get a response. Depending on our setup this may go up all the way to 5.5 seconds. Looking at this, I'm sure there's a big tradeoff between quality, cost, and speed.</p>
<p>Please note I've chosen to use API's here rather than doing everything on my own GPU(s). The reason for this is scalability. Since every server is in the US, I think every round-trip shouldn't add more than 100ms, so building our own in-house infra may reduce the total latency with another 300ms. This is next-level complex though to scale, so I don't think we should aim to solve this problem.</p>
<h1 id="high-level-overview">High-level overview</h1>
<p><img src="./tank.png" alt=""></p>
<p>We can either receive incoming calls or create outgoing calls and stream them. According to <a href="https://www.twilio.com/docs/voice/twiml/stream">Twilios guide</a> you just need to provide the right TwiML XML to the response and it will start streaming when it's time. You can choose to stream it in two tracks or just stream one of them if you don't need the other. This allows you to differentiate who is who, which is great!</p>
<p>Today I'm going to try to make incoming calls stream. I already have a Dutch phone number that I am renting via Twilio, so I can easily make a call to it. The phone number sends incoming calls to my webhook at <code>/function/receiveTwilioCallWithContextRaw</code>.</p>
<p>What I want the behaviour of incoming calls to be:</p>
<ol>
<li>Say 'Operator' (A way to state the AI is listening, inspired by the Matrix ðŸ˜Ž)</li>
<li>Make sure it will start the stream and doesn't hang up.</li>
<li>Receive the stream in my websocket</li>
<li>Detect silences in the audiostream</li>
<li>Send this audio onwards to whisper after a silence is detected of at least a second.</li>
<li>Stream the transcription, with the right context, into a textual agent chatbot.</li>
<li>Every time we get a sentence back, ask PlayHT to generate an audio for this, and make Twilio play this audio via TwiML.</li>
</ol>
<p>Rougly this can be cut up into two pieces:</p>
<ul>
<li>A) The operator picks up and sets up the audio stream</li>
<li>B) Wrapper around a textual agent chatbot that makes it able to handle realtime audio</li>
</ul>
<p>It is important to note that A is custom for twilio and websockets, while B is something more generic that could also potentially be re-used for calls over the internet or completely offline even, if implemented well.</p>
<p>Let's start with A!</p>
<h1 id="a-operator-setup-the-stream">A) Operator, Setup the Stream</h1>
<p>Let's set up everything in node.js with a temporary local server, similar to https://www.twilio.com/blog/live-transcribing-phone-calls-using-twilio-media-streams-and-google-speech-text</p>
<h1 id="b-realtime-voice-wrapper">B) Realtime Voice Wrapper</h1>
<h2 id="2-transcribe-asap">2. Transcribe Asap</h2>
<ul>
<li>Save incoming audio to a wav in realtime (see https://stackoverflow.com/questions/58439005/is-there-any-way-to-save-mulaw-audio-stream-from-twilio-in-a-file)</li>
<li>Test incoming audio (every 100ms) against node-vad until we detect silence.</li>
<li>If there's a silence, send the speech to a realtime api to immediately to get the transcript back.</li>
<li>If there's a new thing being said, interrupt message-processing and save all deltas as new conversation history</li>
</ul>
<h2 id="3-reply">3. Reply</h2>
<ul>
<li>use <code>processMessage({message, isStream:true, callback })</code></li>
<li>collect callback deltas until a sentence is formed. every sentence can be streamed to TTS (or can be done locally) https://docs.play.ht/docs/getting-started-with-playhts-realtime-streaming-api and https://docs.play.ht/reference/api-generate-tts-audio-stream</li>
<li>the streamed audio can direclty be sent with ws.send (in the right format) to be queued and played</li>
<li>interruption capability should stop sending if you interrupt</li>
</ul>
<p>All in all, this would make for realtime</p>
<h1 id="conclusion">Conclusion</h1>
<p>It will probably take me some long evenings to reach this point. Hold tight!</p>

</body>
</html>
